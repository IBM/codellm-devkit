{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eebee2515df69b96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Using CLDK to explain Java methods\n",
    "\n",
    "In this tutorial, we will use CLDK to explain or generate code summary for all the methods in a Java Application.\n",
    "\n",
    "By the end of this tutorial, you will have code summary for all the methods in a Java application. You'll be able to explore some of the benefits of using CLDK to perform fast and easy program analysis and build a LLM-based code summary generation.\n",
    "\n",
    "You will learn how to do the following:\n",
    "\n",
    "<ol>\n",
    "<li> Create a new instance of the CLDK class.\n",
    "<li> Create an analysis object over the Java application.\n",
    "<li> Iterate over all the files in the project.\n",
    "<li> Iterate over all the classes in the file.\n",
    "<li> Iterate over all the methods in the class.\n",
    "<li> Get the code body of the method.\n",
    "<li> Initialize the treesitter utils for the class file content.\n",
    "<li> Sanitize the class for analysis.\n",
    "</ol>\n",
    "Next, we will write a couple of helper methods to:\n",
    "\n",
    "<ol>\n",
    "<li> Format the instruction for the given focal method and class.\n",
    "<li> Prompts the local model on Ollama.\n",
    "<li> Prints the instruction and LLM output.\n",
    "</ol>"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59d05bbe28e62687"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prequisites\n",
    "\n",
    "Before we get started, let's make sure you have the following installed:\n",
    "\n",
    "<ol>\n",
    "<li> Python 3.11 or later\n",
    "<li> Ollama 0.3.4 or later\n",
    "</ol>\n",
    "We will use ollama to spin up a local granite model that will act as our LLM for this turorial."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92896c8ce12b0e9e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisite 1: Install ollama\n",
    "\n",
    "If you don't have ollama installed, please download and install it from here: [Ollama](https://ollama.com/download).\n",
    "Once you have ollama, start the server and make sure it is running.\n",
    "If you're on MacOS, Linux, or WSL, you can check to make sure the server is running by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bfeb1e1227191e3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "systemctl status ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c53214c8106642ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If not, you may have to start the server manually. You can do this by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34a7b1802be15a3f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "systemctl start ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f60e2d9ec12f0bf6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once ollama is up and running, you can download the latest version of the Granite 8b Instruct model by running the following command:\n",
    "\n",
    "There are other granite versions available, but for this tutorial, we will use the Granite 8b Instruct model. You if prefer to use a different version, you can replace `8b-instruct` with any of the other [versions](https://ollama.com/library/granite-code/tags)."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f629a10841aca9e2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ollama pull granite-code:8b-instruct"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ff900382e86a18e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's make sure the model is downloaded by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d076e98c390591b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ollama run granite-code:8b-instruct \\\"Write a python function to print 'Hello, World!'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7aff854a031589f0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisite 3: Install ollama Python SDK"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "531205b489bbec73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2a749932a800c9d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisite 4: Install CLDK\n",
    "CLDK is avaliable on github at github.com/IBM/codellm-devkit.git. You can install it by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6f42dbd286b3f7a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pip install git+https://github.com/IBM/codellm-devkit.git"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "327e212f20a489d6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Step 1: Get the sample Java application\n",
    "For this tutorial, we will use apache commons cli. You can download the source code to a temporary directory by running the following command:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8ec5b9c837898f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wget https://github.com/apache/commons-cli/archive/refs/tags/rel/commons-cli-1.7.0.zip -O /tmp/commons-cli-1.7.0.zip && unzip -o /tmp/commons-cli-1.7.0.zip -d /tmp"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c196e58b3ce90c34"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The project will now be extracted to `/tmp/commons-cli-rel-commons-cli-1.7.0`. We'll remove these files later, so don't worry about the location."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e875e7ce6db504"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Generate code summary\n",
    "Code summarization or code explanation is a task that converts a code written in a programming language to a natural language. This particular task has several\n",
    "benefits, such as understanding code without looking at its intrinsic details, documenting code for better maintenance, etc. To do that, one needs to\n",
    "understand the basic details of code structure works, and use that knowledge to generate the summary using various AI-based approaches. In this particular\n",
    "example, we will be using Large Language Models (LLM), specifically Granite 8B, an open-source model built by IBM. We will show how easily a developer can use\n",
    "CLDK to expose various parts of the code by calling various APIs without implementing various time-intensive program analyses from scratch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad70b81e8957fc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: Add all the neccessary imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15555404790e1411"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ollama\n",
    "from cldk import CLDK\n",
    "from cldk.analysis import AnalysisLevel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8e5de7e5c68020"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: Formulate the LLM prompt. The prompt can be tailored towards various needs. In this case, we show a simple example of generating summary for each\n",
    "method in a Java class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc4ee9a6d27acc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_inst(code, focal_method, focal_class, language):\n",
    "    \"\"\"\n",
    "    Format the instruction for the given focal method and class.\n",
    "    \"\"\"\n",
    "    inst = f\"Question: Can you write a brief summary for the method `{focal_method}` in the class `{focal_class}` below?\\n\"\n",
    "\n",
    "    inst += \"\\n\"\n",
    "    inst += f\"```{language}\\n\"\n",
    "    inst += code\n",
    "    inst += \"```\" if code.endswith(\"\\n\") else \"\\n```\"\n",
    "    inst += \"\\n\"\n",
    "    return inst"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e23523c71636727"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4e9cb4e4f00b25c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: Create a function to call LLM. There are various ways to achieve that. However, for illustrative purpose, we use ollama, a library to communicate with models downloaded locally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8439be222b5caa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prompt_ollama(message: str, model_id: str = \"granite-code:8b-instruct\") -> str:\n",
    "    \"\"\"Prompt local model on Ollama\"\"\"\n",
    "    response_object = ollama.generate(model=model_id, prompt=message)\n",
    "    return response_object[\"response\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62807e0cbf985ae6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4: Create an object of CLDK and provide the programming language of the source code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1022e86e38e12767"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new instance of the CLDK class\n",
    "cldk = CLDK(language=\"java\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c8bbe4e3244f60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: CLDK uses different analysis engine--Codeanalyzer (built using WALA and Javaparser), Treesitter, and CodeQL (future). By default, codenanalyzer has\n",
    "been selected as the default analysis engine. Also, CLDK support different analysis levels--(a) symbol table, (b) call graph, (c) program dependency graph, and\n",
    "(d) system dependency graph. Analysis engine can be selected using ```AnalysisLevel``` enum. In this example, we will generate summarization of all the methods\n",
    "of an application. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23dd4a6e5d5cb0c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an analysis object over the java application\n",
    "analysis = cldk.analysis(project_path=\"/tmp/commons-cli-rel-commons-cli-1.7.0\", analysis_level=AnalysisLevel.symbol_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd09f5e77d4a68a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6: Iterate over all the class files and create the prompt. In this case, we want to provide a customized Java class in the prompt. For instance,\n",
    "\n",
    "```\n",
    "package com.ibm.org;\n",
    "import A.B.C.D;\n",
    "...\n",
    "public class Foo {\n",
    " // code comment\n",
    " public void bar(){ \n",
    "    int a;\n",
    "    a = baz();\n",
    "    // do something\n",
    "    }\n",
    " private int baz()\n",
    " {\n",
    "    // do something\n",
    " }\n",
    " public String dummy (String a)\n",
    " {\n",
    "    // do somthing\n",
    " }   \n",
    "```\n",
    "Given the above class, let's say we want to generate a summary for the ```bar``` method. To understand what it does, we add the callee of this method in the prompt, which in this case is ```baz```. We also remove imports, comments, etc. All of these are done using a single call to ```sanitize_focal_class``` API. In this process, we also use Treesitter to analyze the code. Once the input code has been sanitized, we call the ```format_inst``` method to create the LLM prompt, which has been passed to ```prompt_ollama``` method to generate the summary using LLM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f148325e92781e13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over all the files in the project\n",
    "for file_path, class_file in analysis.get_symbol_table().items():\n",
    "    class_file_path = Path(file_path).absolute().resolve()\n",
    "    # Iterate over all the classes in the file\n",
    "    for type_name, type_declaration in class_file.type_declarations.items():\n",
    "        # Iterate over all the methods in the class\n",
    "        for method in type_declaration.callable_declarations.values():\n",
    "            # Get code body of the method\n",
    "            code_body = class_file_path.read_text()\n",
    "    \n",
    "            # Initialize the treesitter utils for the class file content\n",
    "            tree_sitter_utils = cldk.tree_sitter_utils(source_code=code_body)\n",
    "    \n",
    "            # Sanitize the class for analysis\n",
    "            sanitized_class = tree_sitter_utils.sanitize_focal_class(method.declaration)\n",
    "    \n",
    "            # Format the instruction for the given focal method and class\n",
    "            instruction = format_inst(\n",
    "                code=sanitized_class,\n",
    "                focal_method=method.declaration,\n",
    "                focal_class=type_name,\n",
    "                language=\"java\"\n",
    "            )\n",
    "    \n",
    "            # Prompt the local model on Ollama\n",
    "            llm_output = prompt_ollama(\n",
    "                message=instruction,\n",
    "                model_id=\"granite-code:20b-instruct\",\n",
    "            )\n",
    "    \n",
    "            # Print the instruction and LLM output\n",
    "            print(f\"Instruction:\\n{instruction}\")\n",
    "            print(f\"LLM Output:\\n{llm_output}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "462ef7dceae367ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
