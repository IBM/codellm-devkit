{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install ollama"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eebee2515df69b96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code summarization or code explanation is a task that converts a code written in a programming language to a natural language. This particular task has several\n",
    "benefits, such as understanding code without looking at its intrinsic details, documenting code for better maintenance, etc. To do that, one needs to\n",
    "understand the basic details of code structure works, and use that knowledge to generate the summary using various AI-based approaches. In this particular\n",
    "example, we will be using Large Language Models (LLM), specifically Granite 8B, an open-source model built by IBM. We will show how easily a developer can use\n",
    "CLDK to expose various parts of the code by calling various APIs without implementing various time-intensive program analyses from scratch."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ad70b81e8957fc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 1: Add all the neccessary imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15555404790e1411"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ollama\n",
    "from cldk import CLDK\n",
    "from cldk.analysis import AnalysisLevel"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e8e5de7e5c68020"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 2: Formulate the LLM prompt. The prompt can be tailored towards various needs. In this case, we show a simple example of generating summary for each\n",
    "method in a Java class"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ffc4ee9a6d27acc2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def format_inst(code, focal_method, focal_class, language):\n",
    "    \"\"\"\n",
    "    Format the instruction for the given focal method and class.\n",
    "    \"\"\"\n",
    "    inst = f\"Question: Can you write a brief summary for the method `{focal_method}` in the class `{focal_class}` below?\\n\"\n",
    "\n",
    "    inst += \"\\n\"\n",
    "    inst += f\"```{language}\\n\"\n",
    "    inst += code\n",
    "    inst += \"```\" if code.endswith(\"\\n\") else \"\\n```\"\n",
    "    inst += \"\\n\"\n",
    "    return inst"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e23523c71636727"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4e9cb4e4f00b25c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 3: Create a function to call LLM. There are various ways to achieve that. However, for illustrative purpose, we use ollama, a library to communicate with models downloaded locally."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd8439be222b5caa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prompt_ollama(message: str, model_id: str = \"granite-code:8b-instruct\") -> str:\n",
    "    \"\"\"Prompt local model on Ollama\"\"\"\n",
    "    response_object = ollama.generate(model=model_id, prompt=message)\n",
    "    return response_object[\"response\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62807e0cbf985ae6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 4: Create an object of CLDK and provide the programming language of the source code."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1022e86e38e12767"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a new instance of the CLDK class\n",
    "cldk = CLDK(language=\"java\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2c8bbe4e3244f60"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 5: CLDK uses different analysis engine--Codeanalyzer (built using WALA and Javaparser), Treesitter, and CodeQL (future). By default, codenanalyzer has\n",
    "been selected as the default analysis engine. Also, CLDK support different analysis levels--(a) symbol table, (b) call graph, (c) program dependency graph, and\n",
    "(d) system dependency graph. Analysis engine can be selected using ```AnalysisLevel``` enum. In this example, we will generate summarization of all the methods\n",
    "of an application. To select the application location, you can set the environment variable ```JAVA_APP_PATH```. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23dd4a6e5d5cb0c5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create an analysis object over the java application\n",
    "analysis = cldk.analysis(project_path=\"JAVA_APP_PATH\", analysis_level=AnalysisLevel.symbol_table)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fdd09f5e77d4a68a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Step 6: Iterate over all the class files and create the prompt. In this case, we want to provide a customized Java class in the prompt. For instance,\n",
    "\n",
    "```\n",
    "package com.ibm.org;\n",
    "import A.B.C.D;\n",
    "...\n",
    "public class Foo {\n",
    " // code comment\n",
    " public void bar(){ \n",
    "    int a;\n",
    "    a = baz();\n",
    "    // do something\n",
    "    }\n",
    " private int baz()\n",
    " {\n",
    "    // do something\n",
    " }\n",
    " public String dummy (String a)\n",
    " {\n",
    "    // do somthing\n",
    " }   \n",
    "```\n",
    "Given the above class, let's say we want to generate a summary for the ```bar``` method. To understand what it does, we add the callee of this method in the prompt, which in this case is ```baz```. We also remove imports, comments, etc. All of these are done using a single call to ```sanitize_focal_class``` API. In this process, we also use Treesitter to analyze the code. Once the input code has been sanitized, we call the ```format_inst``` method to create the LLM prompt, which has been passed to ```prompt_ollama``` method to generate the summary using LLM."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f148325e92781e13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Iterate over all the files in the project\n",
    "for file_path, class_file in analysis.get_symbol_table().items():\n",
    "    class_file_path = Path(file_path).absolute().resolve()\n",
    "    # Iterate over all the classes in the file\n",
    "    for type_name, type_declaration in class_file.type_declarations.items():\n",
    "        # Iterate over all the methods in the class\n",
    "        for method in type_declaration.callable_declarations.values():\n",
    "            # Get code body of the method\n",
    "            code_body = class_file_path.read_text()\n",
    "    \n",
    "            # Initialize the treesitter utils for the class file content\n",
    "            tree_sitter_utils = cldk.tree_sitter_utils(source_code=code_body)\n",
    "    \n",
    "            # Sanitize the class for analysis\n",
    "            sanitized_class = tree_sitter_utils.sanitize_focal_class(method.declaration)\n",
    "    \n",
    "            # Format the instruction for the given focal method and class\n",
    "            instruction = format_inst(\n",
    "                code=sanitized_class,\n",
    "                focal_method=method.declaration,\n",
    "                focal_class=type_name,\n",
    "                language=\"java\"\n",
    "            )\n",
    "    \n",
    "            # Prompt the local model on Ollama\n",
    "            llm_output = prompt_ollama(\n",
    "                message=instruction,\n",
    "                model_id=\"granite-code:20b-instruct\",\n",
    "            )\n",
    "    \n",
    "            # Print the instruction and LLM output\n",
    "            print(f\"Instruction:\\n{instruction}\")\n",
    "            print(f\"LLM Output:\\n{llm_output}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "462ef7dceae367ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
